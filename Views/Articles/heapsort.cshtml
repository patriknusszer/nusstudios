@section keywords {
    <meta name="keywords" content="heapsort, onlogn, binary heap, max min heap, heapify" />
}

@section topheading
{
    Heaposrt
}

@section subheading
{
    O(nlogn) sorting algorithm
}

@section description
{
    <article>
        Heapsort was invented by John William Joseph Williams<br/>
        <br/>
        Heapsort has a deterministic O(nlogn) running time, with this being one of the 3 fastest sorting algorithms, and among those the slowest due to the cache misses caused by the arbitrary array jumps.<br/>
        Unlike the other 2 O(nlogn) sorting algorithms, heapsort does not implement the divide and conquer approach.<br/>
        It is an in-place (and instable) sort, which operates on the same input array making it use O(1) extra memory, but handles it like it was a complete binary tree, a maximum or minimum heap.<br/>
        This is the thing which makes it so fast, so much faster than a selection sort, for example.<br/>
        <br/>
        A complete binary tree is a binary tree in which every level is complete, and only the lowest level can be an exception from this, which must fill the leftmost part of that level given its indeed incomplete.<br/>
        The nodes in this complete binary tree are in the same order as the characters of the text of a page in a book as you are reading it.<br/>
        But here, instead of lines of text, you have levels.<br/>
        The top node or root is at position 0 = (2^0 - 1), then its leftmost child is at position 1 = (2^1 - 1), and then its rightmost child is at position 2.<br/>
        Following that analogy, the leftmost child of the leftmost child of the root node is at position 3 (= 2^2 - 1), and the leftmost child of that node must be then at position 7 (= 8 - 1).<br/>
        That is because, in order to get the position of leftmost node on level n, you need to calculate the number of nodes on all previous levels, which are powers of 2:<br/>
        <br/>
        2^0 + 2^1 + 2^2 + 2^3 + ... + 2^(n-1) = leftmost node on level n<br/>
        <br/>
        Note: both the nodes and levels are indexed based on zero, meaning that n = 0 means level 1<br/>
        <br/>
        So for example, if n = 1 (level 2), then index = 2^0 = 1, because we add up the level 1 only.<br/>
        Or for example, if n = 3 (level 4), then index = 2^0 + 2^1 + 2^2 = 7, because we add up level 1, level 2, and level 3 to get index of first node at level 4.<br/>
        <br/>
        This means that, if we are looking at the indices of the leftmost nodes on each level, we see powers of 2 minus one: 0, 1, 3, 7, 15, etc...<br/>
        Can we prove that adding up consecutive powers of 2 starting from the zeroth results in a power of 2 minus 1?<br/>
        <br/>
        The proof is the same used for proving the height of an AVL (Adelson-Velsky-Landis) tree.<br/>
        <br/>
        2^0 + 2^1 + 2^2 + 2^3 + 2^4 + ... + 2^(n-1) = x<br/>
        2^1 + 2^1 + 2^2 + 2^3 + 2^4 + ... + 2^(n-1) = x + 1<br/>
        2 * (2^1 + 2^1 + 2^2 + 2^3 + ... + 2^(n-2)) = x + 1<br/>
        2 * (2 * (2^1 + 2^1 + 2^2 + ... + 2^(n - 3))) = x + 1<br/>
        2^n = x + 1<br/>
        <br/>
        That is because we can always turn an addition into a multiplication, so our final product will have just as many factors as many additives there were, and we had n such additives.<br/>
        <br/>
        Which means 2^n - 1 = x = index of leftmost child on level 'n'<br/>
        <br/>
        Okay, so why am I showing all this?<br/>
        Because in heapsort we will only want to access a given node's immediate children, which we can do in constant time with a single, simple expression.<br/>
        <br/>
        We know 2 things:<br/>
        - The immediate child is on the one lower immediate level<br/>
        - The immediate left child is preceded by exactly 2 times the number of nodes left to its parent, because all of them have exactly 2 children on the 1 lower immediate level<br/>
        <br/>
        So let's say that we have a node at position 'p'.<br/>
        We have this resolution for 'p':<br/>
        (2^n - 1) + x = p, and p < 2^(n + 1) - 1<br/>
        Where n is the index of the level on which the node resides.<br/>
        Where x is the index of the node inside its own level, or the number of nodes to its left on its level.<br/>
        <br/>
        In order to get to the left child node of a node on level n:<br/>
        - You will want to calculate the index of the leftmost child on level (n + 1), which is same as number of nodes above level (n + 1). This can be gained by multiplying the index of the leftmost child on level 'n' (which is same as number of nodes above level n) by 2, because you know that the difference is roughly just one less or one larger power of 2.<br/>
        More precisely, you will need to add +1 after the multiplication, because (2^n - 1) * 2 = 2^(n+1) - 2, which is just 1 less than the desired leftmost nodes's index on the 1 lower immediate level.<br/>
        - You will want to add to the above sum the double of 'x' as well, because the left immediate child is preceded by exactly twice as many nodes as its parent is preceded by on its won level.<br/>
        <br/>
        So the desired index is basically the position 'p' multiplied by 2 plus 1:<br/>
        2 * p = 2 * (2^n - 1) + 2 * x = 2 * ((2^n - 1) + x)<br/>
        2 * p = 2^(n+1) - 2 + 2x<br/>
        2 * p + 1 = 2(n+1) - 1 + 2x<br/>
        <br/>
        And that is the proof.<br/>
        Because 2^n - 1 is the position of the leftmost node of that level on which the parent node is, and therefore 2^(n+1) - 1 is the position of the leftmost node of the level just below it.<br/>
        And you need to add 2 times x more to that, because x is the number of nodes which precede the parent node, and since it is a complete binary tree whose leftmost part is always complete, 2 children per every node preceding the parent precede its left immediate child.<br/>
        <br/>
        And knowing that (2 * index + 1) equals the left immediate child of the node at a given index, plus 1 will equal the right immediate child.<br/>
        <br/>
        So where does heapsort use that formula?<br/>
        <br/>
        It has a recursive max_heapify() or min_heapify() function.<br/>
        A max_heapify() based heapsort produces a maximum heap, where both children of a node are smaller (or equal), and a min_heapify() based one produces a minimum heap, where the children of a node are both larger (or equal).<br/>
        <br/>
        So in case of a max/min heap, wherever you look, the root node of any subtree of a tree will be the largest/smallest element in that whole subtree.<br/>
        <br/>
        The heapify functions are able to heapify such a (sub)tree in which only one node is at wrong position in relation to other nodes of that (sub)tree.<br/>
        To achieve our goal, the heapification is done in bottom up manner, meaning that this only incorrect node in heapsort is always the root/top node of the considered (sub)tree, so that both branches are valid heaps already.<br/>
        This allows us to advance from one lower level to the higher, because if every subtree rooted at nodes of a given level are correct heaps, then the nodes on the one higher level will have both branches heapified so that you can heapify each of them.<br/>
        <br/>
        In worst case, such a heapification takes at worst O(logn) time, where n is number of nodes that heapified subtree, and logn is the height of the subtree.<br/>
        <br/>
        And so such a heapification takes O(logn) because what it does is just swapping nodes vertically, and never steps backwards, hence O(logn).<br/>
        <br/>
        On first occasion, you will need to perform heapification on every array element, starting from the nodes of the lowest level (minus 1 of course, because lowest level only consists of leaf nodes), so that you can heapify larger and larger subtrees based on the already heapified smaller subtrees on the lower level.<br/>
        <br/>
        This initial heapification will take you of course O(nlogn), since you are performing O(logn) operations for almost every node.<br/>
        <br/>
        Let's say you have the following tree with the following nodes:<br/>
        <br/>
        <br/>
        A                   (1)<br/>
        BC                  (2)<br/>
        DEFG                (4)<br/>
        HIJKLMNO            (8)<br/>
        PQRSTUVWXYZ01234    (16)<br/>
        <br/>
        First, you would heapify the subtrees HIJKLMNO, and you can do so, since by definition, their children are already heapified, since they are not real subtrees, they are all leaf nodes.<br/>
        Now you could then heapify subtrees DEFG because subtrees HIJKLMNO are already heapified.<br/>
        Then, you could heapify subtrees BC, because subtrees DEFG are already heapified.<br/>
        And finally, you could then heapify tree A, because subtrees BC are already heapified.<br/>
        <br/>
        So let's prove heapify functions actually work like that!<br/>
        1) We have an initial state:<br/>
        We have a subtree, and its 2 branches, and in both, the nodes inside are in correct relative position, meaning they are both independent, but are correct binary heaps on their own.<br/>
        <br/>
        2) Prove that, the change to this state produces another correct state, which is 1 correct binary heap with all elements being in correct position relative to each other!<br/>
        <br/>
        A single heapify call will restore a node and its 2 immediate children into correct relative position, meaning it will generate a correct local binary heap at that aggregate of nodes.<br/>
        If we have a maximum heap, it is determined which of the 3 elements is the largest.<br/>
        Given the largest element is one of the children nodes, it takes the top position and the other 2 are made to be the its children:<br/>
        <br/>
        Before:<br/>
        5<br/>
        47<br/>
        <br/>
        After:<br/>
        7<br/>
        45<br/>
        <br/>
        So yes, 1 heapify call indeed succeeds in creating a mini triangle which is a correct binary tree.<br/>
        It is said that, that largest element is bubbled up, while the smaller top element is bubbled down to take its place.
        <br/>
        <br/>
        A heapify can be safely performed on any binary tree, in which there is only 1 node which is in some incorrect position to some other nodes subordinate to it in the heap.<br/>
        First off, of course, this incorrect node is the apex of the subtree which we are looking at, but whenever it is bubbled down to a lower level, we get another binary heap in which all nodes except that node are in correct position relative to each other, and the bubbled down node may only be in incorrect position relative to its subordinates - so it is always in correct position relative to its superordinates inside that subtree - until that we will continue bubbling it down, until it either hits the end of the tree at which point it simply can not be any longer at incorrect position relative to subordinates, or until, the 1 subordinate is found to be in correct position relative to it, at which point all indirect subordinates must also be in correct position relative to it.
        <br/>
        <br/>
        1) Here, safe means on 1 hand, that bubbling up a node does not put that node into an incorrect position relative to any other (sub)tree nodes, if it was in correct position to all of them before, since its former immediate children are still subordinate to it, but now indirectly, and all its former indirectly superordinate nodes are still superordinate, the closest now being immediate parent to it.<br/>
        2) Safe means, on the other hand, bubbling down a node does not put that node into an incorrect position relative to its former superordinate nodes, which are still superordinate to it, though all of them indirectly.<br/>
        3) Safe also means all other nodes stayed at their place, therefore they are at correct positions relative to each other
        4) However, the bubbled down node can still be at incorrect position relative to its subordinate nodes
        <br/>
        <br/>
        That means, whenever we call heapification for the same node, it turns a subordinate which is in incorrect position relative to it into its immediate parent whcih must be at correct position now relative to it, and it is recursively applied up until:<br/>
        <br/>
        1) none of the new immediate children of the bubbled down node are in incorrect position relative to it, which of course also means none of its indirect children are at incorrect position relative to it<br/>
        2) it has no children, because we have arrived at the end of the tree, so it must be at correct position<br/>
        <br/>
        That means, this constantly bubbled down node eventually gets into correct position relative to all other elements in that subtree which we were looking that, so the change made to our initial state indeed produces another correct state.
        <br/>
        <br/>
        Since we are constantly bubbling down a node vertically, it is also proven that in worst case we are doing as many operations as tall the subtree was, which is O(logn) in worst case.<br/>
        <br/>
        So we can build a binary heap out of our input array in O(nlogn).<br/>
        <br/>
        Building a minimum heap is exactly the same story, but instread you are looking for smallest element out of the 3 and bubble that up, while bubbling the apex down to its place.
        <br/>
        <br/>
        But that is only half of the story.<br/>
        <br/>
        Now you are sure that the largest element of the array is the apex.<br/>
        That is the zeroth index of the array.<br/>
        It would be too easy to just put the resulting numbers into a new array.<br/>
        We are going to do it in place.<br/>
        You swap this apex with the element at the last array index, wich is the rightmost node on the lowest level.<br/>
        Now you heapify the tree at its apex again, to bubble down that small number you have just put there, but you specify a 1 smaller array length, so that heapify is not going to bother the largest number you have placed at the largest index.<br/>
        After heapification, you will have the largest node of the 1 smaller tree at the top position, but since this new tree was basically the largest plus all the rest, the largest of the rest is going to be the second largest.<br/>
        And again, you swap the apex with the node at the end of the last array index minus one, which is anyway the end of the 1 smaller heap we are considering in this second step.<br/>
        And you continue doing it, until you have an array with a length of at least 3 nodes.<br/>
        <br/>
        <br/>
        So this second part of heapsort will take you another O(nlogn) operations, and since you have O(nlogn) for first part, and O(nlogn) for second, you also have eventually O(nlogn).
        <br/>
    </article>
}

@section btnname
{

}

@section url
{

}
