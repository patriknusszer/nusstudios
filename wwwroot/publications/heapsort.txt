Heapsort was invented by John William Joseph Williams

Heapsort has a deterministic O(nlogn) running time, with this being one of the 3 fastest sorting algorithms, and among those the slowest due to the cache misses caused by the arbitrary array jumps.
Unlike the other 2 O(nlogn) sorting algorithms, heapsort does not implement the divide and conquer approach.
It is an in-place (and instable) sort, which operates on the same input array making it use O(1) extra memory, but handles it like it was a complete binary tree, a maximum or minimum heap.
This is the thing which makes it so fast, so much faster than a selection sort, for example.

A complete binary tree is a binary tree in which every level is complete, and only the lowest level can be an exception from this, which must fill the leftmost part of that level given its indeed incomplete.
The nodes in this complete binary tree are in the same order as the characters of the text of a page in a book as you are reading it.
But here, instead of lines of text, you have levels.
The top node or root is at position 0 = (2^0 - 1), then its leftmost child is at position 1 = (2^1 - 1), and then its rightmost child is at position 2.
Following that analogy, the leftmost child of the leftmost child of the root node is at position 3 (= 2^2 - 1), and the leftmost child of that node must be then at position 7 (= 8 - 1).
That is because,Â in order to get the position of leftmost node on level n, you need to calculate the number of nodes on all previous levels, which are powers of 2:

2^0 + 2^1 + 2^2 + 2^3 + ... + 2^(n-1) = leftmost node on level n

Note: both the nodes and levels are indexed based on zero, meaning that n = 0 means level 1

So for example, if n = 1 (level 2), then index = 2^0 = 1, because we add up the level 1 only.
Or for example, if n = 3 (level 4), then index = 2^0 + 2^1 + 2^2 = 7, because we add up level 1, level 2, and level 3 to get index of first node at level 4.

This means that, if we are looking at the indices of the leftmost nodes on each level, we see powers of 2 minus one: 0, 1, 3, 7, 15, etc...
Can we prove that adding up consecutive powers of 2 starting from the zeroth results in a power of 2 minus 1?

The proof is the same used for proving the height of an AVL (Adelson-Velsky-Landis) tree.

2^0 + 2^1 + 2^2 + 2^3 + 2^4 + ... + 2^(n-1) = x
2^1 + 2^1 + 2^2 + 2^3 + 2^4 + ... + 2^(n-1) = x + 1
2 * (2^1 + 2^1 + 2^2 + 2^3 + ... + 2^(n-2)) = x + 1
2 * (2 * (2^1 + 2^1 + 2^2 + ... + 2^(n - 3))) = x + 1
2^n = x + 1

That is because we can always turn an addition into a multiplication, so our final product will have just as many factors as many additives there were, and we had n such additives.

Which means 2^n - 1 = x = index of leftmost child on level 'n'

Okay, so why am I showing all this?
Because in heapsort we will only want to access a given node's immediate children, which we can do in constant time with a single, simple expression.

We know 2 things:
- The immediate child is on the one lower immediate level
- The immediate left child is preceded by exactly 2 times the number of nodes left to its parent, because all of them have exactly 2 children on the 1 lower immediate level

So let's say that we have a node at position 'p'.
We have this resolution for 'p':
(2^n - 1) + x = p, and p < 2^(n + 1) - 1
Where n is the index of the level on which the node resides.
Where x is the index of the node inside its own level, or the number of nodes to its left on its level.

In order to get to the left child node of a node on level n:
- You will want to calculate the index of the leftmost child on level (n + 1), which is same as number of nodes above level (n + 1). This can be gained by multiplying the index of the leftmost child on level 'n' (which is same as number of nodes above level n) by 2, because you know that the difference is roughly just one less or one larger power of 2.
More precisely, you will need to add +1 after the multiplication, because (2^n - 1) * 2 = 2^(n+1) - 2, which is just 1 less than the desired leftmost nodes's index on the 1 lower immediate level.
- You will want to add to the above sum the double of 'x' as well, because the left immediate child is preceded by exactly twice as many nodes as its parent is preceded by on its won level.

So the desired index is basically the position 'p' multiplied by 2 plus 1:
2 * p = 2 * (2^n - 1) + 2 * x = 2 * ((2^n - 1) + x)
2 * p = 2^(n+1) - 2 + 2x
2 * p + 1 = 2(n+1) - 1 + 2x

And that is the proof.
Because 2^n - 1 is the position of the leftmost node of that level on which the parent node is, and therefore 2^(n+1) - 1 is the position of the leftmost node of the level just below it.
And you need to add 2 times x more to that, because x is the number of nodes which precede the parent node, and since it is a complete binary tree whose leftmost part is always complete, 2 children per every node preceding the parent precede its left immediate child.

And knowing that (2 * index + 1) equals the left immediate child of the node at a given index, plus 1 will equal the right immediate child.

So where does heapsort use that formula?

It has a recursive max_heapify() or min_heapify() function.
A max_heapify() based heapsort produces a maximum heap, where both children of a node are smaller (or equal), and a min_heapify() based one produces a minimum heap, where the children of a node are both larger (or equal).

So in case of a maximum/minimum heap, wherever you look, the root node of any subtree of a tree will be the largest/smallest element in that whole subtree.

The heapify functions are able to heapify such a (sub)tree in which there is a bad node, where by bad node I mean that all other nodes are at correct position relative to the rest of the nodes, and only either the descendant or ascendant nodes below or above the bad node may be at wrong position, and if so only relative to this one bad node.
A node is considered to be at wrong position given not all nodes above it are all larger/smaller, and or not all nodes below it are all smaller/larger.

For the sake of simplicity and tradition let's now only consider that case when the nodes which are in wrong position with the bad node are below it.

One important property of this case is that the subtrees rooted at the children of the bad node are valid binary heaps.
That is the information we use to sort with heapify functions methodically.

1) We have an initial state: the single element trees rooted at the nodes of the lowest level which are by definition already valid binary heaps.
Our state always have valid binary trees rooted at the nodes of some level which is true initally as well.

2) Our change involves heapifying the nodes on the level which is just above the level of the nodes at which the valid binary heaps of our current state are rooted. Heapification will succeed since the heapified nodes have valid binary trees to their left and right. Now our state is considered to consist of the newly produced valid binary heaps rooted at the nodes of this one upper level.

So in simple terms we can heapify the subtrees rooted at the nodes of some level 'n' given the subtrees rooted at the nodes of level 'n - 1' are valid binary heaps if they indeed have children on teh previous lower level.
So we are basically "heapifying" the levels of our tree in bottom up manner recurcively.

A (1)
BC (2)
DEFG (4)
HIJKLMNO (8)
PQRSTUVWXYZ01234 (16)

First, you would heapify the subtrees HIJKLMNO, and you can do so, since by definition, their children are already heapified, since they are not real subtrees, they are all leaf nodes.
Now you could then heapify subtrees DEFG because subtrees HIJKLMNO are already heapified.
Then, you could heapify subtrees BC, because subtrees DEFG are already heapified.
And finally, you could then heapify tree A, because subtrees BC are already heapified.

I only need to exaplin now how exactly heapification works.
So presume we have some tree rooted at a node 'a', then its children (if it has) are valid binary heaps, and so 'a' must be the bad boy here who requires some correction...
That basically means 'a' is not maximum/minimum node of the subtree rooted at 'a'. That's problem.
What we do is select the largest/smallest element out of 'a' and its children and swap it with with the apex (=root).
That will produce as a nice little triangle which is a correct binary heap on its own.
This swapping is a very safe operation:

1) bubbling up the maximum/minimum element will keep its original direct descendants - and their descendants - which are now indirect descendants smaller/larger, and its new direct descendants - and their descendants - will also be smaller/larger as required.
2) bubbling down the "bad" node below the apex resolves the wrong relationship between them.
3) the last question is: is the bad node still bad at its new place relative to the nodes below it?
If not, we are finished with heapification.
If yes, we will call another heapify again on the same bad node, and please note:
4) bubbling down the bad node maintains the correct relationship with its former direct ascendant - and its ascendants - if it existed, as required.
5) bubbling up the maximum/minimum element will keep its original indirect ascendants larger/smaller as required.
These latter 2 are important things to remember on each recursive call starting from the 2nd, because on the first call we do not care about the ascendants.
6) So in summary, in worst case, you will be calling heapify again and again till it still has children relative to which it can be "bad" / can be at wrong position.

In summary, as you are moving down your bad node you are always restoring a wrong positional relationship between the bad number and a node below it, while preserving all the former correct positional relationships between nodes.
Now you can see that the algorithm is constantly moving downwards vertically meaning that a heapify will take you at worst O(log(n)) where n is the number of nodes in the subtree currently being considered/heapified.

And you know that we call heapify in the first part of our algorithm for almost all nodes of the complete tree you get O(nlog(n)) for the first part. That is what it takes to turn your array into a binary heap.

But that is only half of the story.

Now you are sure that the largest element of the array is the apex.
That is the zeroth index of the array.
It would be too easy to just put the resulting numbers into a new array.
We are going to do it in place.
You swap this apex with the element at the last array index, wich is the rightmost node on the lowest level.
Now you heapify the tree at its apex again, to bubble down that small number you have just put there, but you specify a 1 smaller array length, so that heapify is not going to bother the largest number you have placed at the largest index.
After heapification, you will have the largest node of the 1 smaller tree at the top position, but since this new tree was basically the largest plus all the rest, the largest of the rest is going to be the second largest.
And again, you swap the apex with the node at the end of the last array index minus one, which is anyway the end of the 1 smaller heap we are considering in this second step.
And you continue doing it, until you have an array with a length of at least 3 nodes.


So this second part of heapsort will take you another O(nlogn) operations, and since you have O(nlogn) for first part, and O(nlogn) for second, you also have eventually O(nlogn). 